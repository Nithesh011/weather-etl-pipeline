{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cnS319JLX8uP"
      },
      "outputs": [],
      "source": [
        "#Install Libraries\n",
        "!pip install pyspark google-cloud-bigquery requests google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2: Extract (API Fetch with GCS Backup)\n",
        "import requests\n",
        "import json\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "\n",
        "url = \"https://api.open-meteo.com/v1/forecast?latitude=19.07&longitude=72.88&daily=temperature_2m_max,temperature_2m_min,precipitation_sum&timezone=Asia/Kolkata&past_days=3\"  # Mumbai, India\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    # Save to local temp file\n",
        "    with open('weather_data.json', 'w') as f:\n",
        "        json.dump(data, f)\n",
        "    # Save to GCS for backup\n",
        "    client = storage.Client.from_service_account_json('your-key.json')  # Replace with your local key path\n",
        "    bucket = client.get_bucket('your-bucket')  # Replace with your bucket\n",
        "    blob_name = f'weather_raw_{datetime.now().strftime(\"%Y-%m-%d\")}.json'\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_string(json.dumps(data))\n",
        "    print(f\"Extracted and saved to GCS: {blob_name}\")\n",
        "else:\n",
        "    print(\"API error:\", response.status_code)"
      ],
      "metadata": {
        "id": "JOKCc5IaJy-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 3:Transform (with City and End Date Columns)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, sum, col, lit, max\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'  # Keep as is for testing\n",
        "spark = SparkSession.builder.appName('WeatherETL').config(\"spark.driver.memory\", \"1g\").config(\"spark.executor.memory\", \"1g\").getOrCreate()\n",
        "df = spark.read.json('weather_data.json')\n",
        "df_daily = df.selectExpr(\"explode(arrays_zip(daily.time, daily.temperature_2m_max, daily.temperature_2m_min, daily.precipitation_sum)) as daily_data\")\n",
        "df_exploded = df_daily.select(\n",
        "    col(\"daily_data.time\").alias(\"date\"),\n",
        "    col(\"daily_data.temperature_2m_max\").alias(\"max_temp\"),\n",
        "    col(\"daily_data.temperature_2m_min\").alias(\"min_temp\"),\n",
        "    col(\"daily_data.precipitation_sum\").alias(\"precipitation\")\n",
        ")\n",
        "end_date = df_exploded.agg(max(\"date\").alias(\"end_date\")).collect()[0][\"end_date\"]\n",
        "analysis = df_exploded.agg(\n",
        "    avg(\"max_temp\").alias(\"avg_max_temp\"),\n",
        "    avg(\"min_temp\").alias(\"avg_min_temp\"),\n",
        "    sum(\"precipitation\").alias(\"total_precipitation\")\n",
        ").withColumn(\"city\", lit(\"Mumbai\")).withColumn(\"end_date\", lit(end_date))\n",
        "analysis.coalesce(1).write.mode('overwrite').csv('transformed_weather', header=True)\n",
        "print(\"Transformed!\")"
      ],
      "metadata": {
        "id": "wFbAEs6dJ7Yx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 4: Load (to BigQuery)\n",
        "from google.cloud import bigquery\n",
        "import os\n",
        "client = bigquery.Client.from_service_account_json('your-key.json')  # Replace with your local key path\n",
        "dataset_id = 'weather_dataset'\n",
        "try:\n",
        "    client.get_dataset(dataset_id)\n",
        "except:\n",
        "    client.create_dataset(dataset_id)\n",
        "table_id = 'your-project-id.weather_dataset.weather_analysis'  # Replace with your Project ID\n",
        "for file in os.listdir('transformed_weather'):\n",
        "    if file.startswith('part-') and file.endswith('.csv'):\n",
        "        csv_file = f'transformed_weather/{file}'\n",
        "        break\n",
        "else:\n",
        "    raise Exception(\"No CSV found in 'transformed_weather'\")\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        "    autodetect=True,\n",
        "    write_disposition='WRITE_TRUNCATE'  # Overwrite to avoid duplicates\n",
        ")\n",
        "with open(csv_file, 'rb') as source_file:\n",
        "    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
        "job.result()\n",
        "print(\"Loaded to BigQuery!\")"
      ],
      "metadata": {
        "id": "b7Rd6_z0KDg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKhm0sPBKtHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}